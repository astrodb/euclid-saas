---
# This name is used for the Heat stack and as a prefix for the
# cluster node hostnames.
cluster_name: openhpc

# This parameter should be set to the name of an RSA keypair you have
# uplaoded to OpenStack.
cluster_keypair: bharat

# Site-specific network configuration.
cluster_net:
  - { net: "ilab", subnet: "ilab" }
  - { net: "p3-bdn", subnet: "p3-bdn" }
  - { net: "p3-lln", subnet: "p3-lln" }

# A 3-NIC node resource in the heat templates.
cluster_nodenet_environment: "nodenet-3.yaml"

# Enable the use of config drive, for managing IP assignment on IPoIB
cluster_config_drive: true

# Multi-node application topology.  In this case we have a SLURM
# deployment formed from a login/controller node and a number of
# batch compute nodes.
cluster_groups:
  - "{{ slurm_login }}"
  - "{{ slurm_compute }}"

# Login node configuration.
slurm_login_flavor: "compute-B"
slurm_login_image: "CentOS7.5-OpenHPC"
slurm_login_num_nodes: 1

slurm_login:
  name: "login"
  flavor: "{{ slurm_login_flavor }}"
  image: "{{ slurm_login_image}}"
  num_nodes: "{{ slurm_login_num_nodes }}"
  user: "centos"

# Compute node configuration.
slurm_compute_flavor: "compute-A"
slurm_compute_image: "CentOS7.5-OpenHPC"
slurm_compute_num_nodes: 8

slurm_compute:
  name: "compute"
  flavor: "{{ slurm_compute_flavor }}"
  image: "{{ slurm_compute_image}}"
  num_nodes: "{{ slurm_compute_num_nodes }}"
  user: "centos"

# Node group assignments for cluster roles.
# These group assignments are appended to the cluster inventory file.
# The names of these roles are cross-referenced to groups referred to
# in playbooks in the ansible/ directory.
cluster_roles:
  - name: "ceph_client"
    groups: "{{ cluster_groups }}"
  - name: "login"
    groups: [ "{{ slurm_login }}" ]
  - name: "batch"
    groups: [ "{{ slurm_compute }}" ]
  - name: "mdadm"
    groups: [ "{{ slurm_compute }}" ]
  - name: "glusterfs_server"
    groups: [ "{{ slurm_compute }}" ]
  - name: "glusterfs_client"
    groups: [ "{{ slurm_compute }}" ]
  - name: "beegfs_mgmt"
    groups: [ "{{ slurm_login }}" ]
  - name: "beegfs_mds"
    groups: [ "{{ slurm_login }}" ]
  - name: "beegfs_oss"
    groups: [ "{{ slurm_compute }}" ]
  - name: "beegfs_client"
    groups:
    - "{{ slurm_login }}"
    - "{{ slurm_compute }}" 

# Define a list of SLURM partitions to create.
openhpc_slurm_partitions: 
  - "{{ slurm_compute }}"

# Choose between glusterfs, beegfs and none.
cluster_fs: beegfs

# Gluster server config
gluster_cluster_volume_name: "{{ cluster_name }}"
gluster_cluster_block_devices:
- md0
gluster_cluster_transport_interface: ib0
gluster_cluster_transport_mode: rdma
gluster_cluster_force_format: no
gluster_cluster_volume_options:
  cluster.nufa: 'on'
  performance.cache-size: '10GB'
  diagnostics.brick-log-level: 'WARNING'

# Gluster client config
gluster_src: "localhost:/{{ gluster_cluster_volume_name }}"
gluster_mnt: "/mnt/{{ gluster_cluster_volume_name }}"

# Config for Ceph mount
ceph_mount_share_name: HomeDirs
ceph_mount_path: /alaska
ceph_mount_fuse: true

# BeegFS config
beegfs_block_devices:
- md0
beegfs_port_oss: [8003]
beegfs_fstype: "xfs"
beegfs_force_format: no
beegfs_host_mgmt: "{{ groups['cluster_beegfs_mgmt'] | first }}"
beegfs_interfaces: ["ib0"]
beegfs_path_meta: "/data/beegfs/beegfs_meta"
beegfs_rdma: yes
beegfs_state: present

# BeegFS client config
beegfs_client_confs:
- beegfs_path_client: "/mnt/{{ cluster_name }}"
  beegfs_host_mgmt: "{{ beegfs_host_mgmt }}"
  beegfs_port_client: 8004
- beegfs_path_client: /mnt/storage-ssd
  beegfs_host_mgmt: storage-ssd-node-0
  beegfs_port_client: 18004
- beegfs_path_client: /mnt/storage-nvme
  beegfs_host_mgmt: storage-nvme-node-0
  beegfs_port_client: 28004

# Software defined raid config
md0:
  # Define array name
  name: 'md0'
  # Define disk devices to assign to array
  devices:
    - '/dev/sdb'
    - '/dev/sdc'
    - '/dev/sdd'
  # Define filesystem to partition array with
  filesystem: 'xfs'
  filesystem_opts: ''
  # Define the array raid level
  # 0|1|4|5|6|10
  level: '0'
  # Define if array should be present or absent
  state: 'present'

mdadm_arrays:
  - "{{ md0 }}"

# A list of OpenHPC runtime libraries to install on compute and control nodes
openhpc_packages:
  - cfitsio
  - cfitsio-devel
  - wcslib
  - wcslib-utils
  - wcslib-devel
  - gcc-gfortran 
  - gcc-c++
  - ncurses
  - ncurses-devel
  - readline
  - readline-devel
  - python-devel
  - git-lfs
...
