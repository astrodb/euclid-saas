---
# This name is used for the Heat stack and as a prefix for the
# cluster node hostnames.
cluster_name: "euclid_{{ infra_name }}"

cluster_groups:
  - "{{ openvpn_gw }}"
  - "{{ slurm_login }}"
  - "{{ slurm_control }}"
  - "{{ slurm_compute }}"
  - "{{ cvmfs_proxy }}"

cluster_gw_group: "gw"
cluster_guest_user: "{{ infra_user_base }}"
cluster_homedir: "/euclid"

openvpn_gw:
  name: "gw"
  flavor: "{{ infra_flavor_base }}"
  image: "{{ infra_image_base }}"
  user: "{{ infra_user_base }}"
  num_nodes: 1
  nodenet_resource: "{{ infra_resource_net_external }}"
  nodenet_fips:
    # FIXME
    - uuid: "1eacdf5a-7681-4a9f-ae4e-c16474bc7717"
      ip: "130.246.215.191"

slurm_login:
  name: "login"
  flavor: "{{ infra_flavor_login }}"
  image: "{{ infra_image_login }}"
  user: "{{ infra_user_login }}"
  num_nodes: 1
  nodenet_resource: "{{ infra_resource_net_external }}"
  nodenet_fips:
    # FIXME
    - uuid: "4996ee00-1ee3-4db3-ab28-ec8e5acbd256"
      ip: "130.246.215.152"

slurm_control:
  name: "control"
  flavor: "{{ infra_flavor_base }}"
  image: "{{ infra_image_base }}"
  user: "{{ infra_user_base }}"
  num_nodes: 1
  nodenet_resource: "{{ infra_resource_net_base }}"

slurm_compute:
  name: "compute"
  flavor: "{{ infra_flavor_compute }}"
  image: "{{ infra_image_compute }}"
  user: "{{ infra_user_compute }}"
  num_nodes: 2
  nodenet_resource: "{{ infra_resource_net_base }}"
  node_resource: "{{ infra_resource_node_storage }}"
  volume_size: 100
  volume_type: "rbd"

cvmfs_proxy:
  name: "proxy"
  flavor: "{{ infra_flavor_base }}"
  image: "{{ infra_image_base }}"
  user: "{{ infra_user_base }}"
  num_nodes: 1
  nodenet_resource: "{{ infra_resource_net_base }}"
  node_resource: "{{ infra_resource_node_storage }}"
  volume_size: 60
  volume_type: "rbd"


# Node group assignments for cluster roles.
# These group assignments are appended to the cluster inventory file.
# The names of these roles are cross-referenced to groups referred to
# in playbooks in the ansible/ directory.
cluster_roles:
  # A group of all nodes deployed in this infrastructure.
  - name: "{{ infra_name }}"
    groups:
      - "{{ openvpn_gw }}"
      - "{{ slurm_login }}"
      - "{{ slurm_control }}"
      - "{{ slurm_compute }}"
      - "{{ cvmfs_proxy }}"

  # All nodes that should be mounting the Ceph filesystem
  - name: "ceph_client"
    groups: 
      - "{{ slurm_login }}"
      - "{{ slurm_control }}"
      - "{{ slurm_compute }}"

  - name: "control"
    groups: 
      - "{{ slurm_control }}"
  - name: "login"
    groups: 
      - "{{ slurm_login }}"
  - name: "batch"
    groups: 
      - "{{ slurm_compute }}" 
  - name: "cvmfs_proxy"
    groups:
      - "{{ cvmfs_proxy }}"
  - name: "ceph_mon"
    groups:
      - "{{ slurm_login }}"
      - "{{ slurm_control }}"
      - "{{ openvpn_gw }}"
  - name: "ceph_osd"
    groups:
      - "{{ slurm_compute }}"

# Define a list of SLURM partitions to create.
openhpc_slurm_partitions: 
  - name: "{{ infra_name }}"
    groups:
      - "{{ slurm_compute }}"


